{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import spacy\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "data = requests.get(\"https://www.gutenberg.org/files/1342/1342-h/1342-h.htm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"prideandprejudice.txt\",'wb') as f:\n",
    "   f.write(data.content) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = open(\"Book.txt\",\"w\",encoding=\"utf8\")\n",
    "input = open(\"prideandprejudice.txt\",encoding=\"utf8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for line in input:\n",
    "    output.write(re.sub(r'<.*?>', r'', line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "input.close()\n",
    "output.close()\n",
    "\n",
    "open_file = open(\"Book.txt\",\"r\",encoding='utf8')\n",
    "file = open_file.read()\n",
    "doc = nlp(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of tokens:  155973\n",
      "number of verbs: 17975\n",
      "most common used entity and the count: [('Elizabeth', 564)]\n",
      "number of sentences: 7274\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "token_list = []\n",
    "for token in doc:\n",
    "    token_list.append(token.text)\n",
    "#print(token_list)\n",
    "print(\"number of tokens: \", len(token_list))\n",
    "\n",
    "verbs_list = []\n",
    "for verb in doc:\n",
    "    if verb.pos_ == \"VERB\":\n",
    "        verbs_list.append(verb.text)\n",
    "print(\"number of verbs:\" ,len(verbs_list))\n",
    "\n",
    "entity_list = []\n",
    "for entity in doc.ents:\n",
    "    entity_list.append(entity.text.strip())\n",
    "print(\"most common used entity and the count:\" , Counter(entity_list).most_common(1))\n",
    "sentence_list = []\n",
    "for sent in doc.sents:\n",
    "    sentence_list.append(sent.text)\n",
    "print(\"number of sentences:\",len(sentence_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 1: Most people start at our Web site which has the main PG search facility:\n",
      "\n",
      "     \n",
      "\n",
      "Sentence 2: Most people start at our Web site which has the main PG search facility:\n",
      "\n",
      "     \n",
      "\n",
      "Similarity: 0.8281176090240479\n"
     ]
    }
   ],
   "source": [
    "#Finding the two texts that are at least 10 words in length, and are most similar (but not identical)    \n",
    "    \n",
    "# Making a list of sentences with at least 10 words\n",
    "long_sents = [sent for sent in list(doc.sents) if len(sent) >= 10]\n",
    "\n",
    "# Loop across each combination of sentences in document\n",
    "high_sim = 0\n",
    "\n",
    "for i in range(len(long_sents)-1):\n",
    "    for j in range(i+1,len(long_sents)-1):\n",
    "        s1 = long_sents[i]\n",
    "        s2 = long_sents[j]\n",
    "        \n",
    "sim = s1.similarity(s2) #computes the similarity\n",
    "        \n",
    "# If new high for similarity is found store indices. 0.99 is used as an upper bound to avoid catching nearly \n",
    "#identical texts from the selected file\n",
    "        \n",
    "if sim > high_sim and s1.text != s2.text and sim <.99:\n",
    "            s1_high = i\n",
    "            s2_high = j\n",
    "            high_sim = sim\n",
    "\n",
    "print('Sentence 1: {}'.format(long_sents[s1_high].text))\n",
    "print('\\nSentence 2: {}'.format(long_sents[s2_high].text))\n",
    "print('\\nSimilarity: {}'.format(high_sim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first from sentence 15: This\n",
      "vector of first word: [-8.7595e-02  3.5502e-01  6.3868e-02  2.9292e-01 -2.3635e-01 -6.2773e-02\n",
      " -1.6105e-01 -2.2842e-01  4.1587e-02  2.4844e+00 -3.8217e-01  3.2806e-02\n",
      "  1.2348e-01 -1.8422e-03 -1.3848e-01 -1.0005e-03 -4.3081e-02  1.1659e+00\n",
      " -4.7327e-02 -5.6004e-02  1.5617e-01 -1.3394e-01  2.3229e-01  8.7602e-02\n",
      " -3.2329e-01  1.6721e-01 -1.6221e-01 -9.1919e-02 -3.8004e-01  1.2686e-01\n",
      "  6.7819e-02  3.2509e-01 -5.7245e-02 -3.2630e-01 -1.1903e-01 -6.3964e-04\n",
      " -5.9275e-03 -2.9934e-01 -8.5043e-02 -2.6683e-01 -1.5815e-01  2.5963e-01\n",
      "  2.2571e-01  6.2582e-02 -1.9394e-01  2.1922e-01 -3.1186e-01  3.7084e-01\n",
      " -3.6577e-01 -5.2483e-02 -4.3101e-01  1.2379e-01  1.5529e-02 -1.2505e-01\n",
      "  2.2327e-01  2.9365e-01 -8.5104e-03 -8.3909e-02  2.4078e-01 -3.4913e-01\n",
      " -2.8355e-01 -7.6594e-02 -1.7130e-01  3.2869e-01  2.9024e-01 -6.2741e-02\n",
      " -5.5278e-02 -2.8706e-01  7.9608e-02  1.3234e-01  4.7857e-01  1.9623e-01\n",
      "  2.7314e-01 -1.3089e-01  2.7630e-01 -8.8846e-02 -1.2379e-01  7.3987e-02\n",
      " -5.1962e-01  3.5227e-01 -2.9182e-02  1.6203e-01 -3.6908e-02  2.8035e-01\n",
      "  3.1739e-01 -2.7597e-01 -4.3637e-01 -3.2842e-01  3.6760e-01 -1.6278e-01\n",
      " -1.6278e-01  3.7066e-01 -1.1340e-01  3.0920e-01  2.6133e-01  3.9483e-01\n",
      " -7.4612e-02 -2.2158e-01  2.5172e-01  2.9990e-01  1.0566e-01 -1.1406e-01\n",
      " -3.5395e-01  6.6704e-02  5.0216e-02 -7.1479e-01  9.8646e-02 -5.8832e-02\n",
      " -4.7790e-03 -2.3920e-01  1.0179e-01 -2.7205e-01  1.6836e-01  2.3420e-01\n",
      " -3.7496e-01  3.1125e-01 -3.1120e-01  2.1778e-01  3.0323e-01 -1.1729e-01\n",
      " -5.4639e-02 -1.5356e-01  5.1771e-02 -1.1426e-01  2.2473e-02  4.4405e-02\n",
      " -3.2101e-01 -2.7799e-01  2.4675e-01 -1.1760e-01 -1.5964e-02 -4.0969e-01\n",
      " -2.6082e-01  1.6021e-01  6.1166e-02 -3.1131e-03 -3.0573e-01 -4.1686e-02\n",
      "  2.3524e-01 -5.8415e-02 -1.6003e+00  1.0126e-01  1.2116e-01  1.5319e-02\n",
      " -1.1945e-01 -3.9095e-01 -1.9919e-01  4.3930e-02  2.2886e-01 -5.3961e-02\n",
      " -2.6570e-02  1.1952e-01  1.8446e-01 -6.9963e-02  3.6429e-01 -2.3679e-02\n",
      " -4.5081e-01 -3.7263e-02 -1.3243e-01 -1.1009e-01 -1.5201e-01  1.2182e-01\n",
      " -9.3379e-02  1.0215e-01 -3.4126e-01 -7.8150e-02  2.7685e-02 -4.8772e-03\n",
      "  2.7281e-01 -1.1304e-01  1.2470e-02  2.7008e-01  3.8885e-01 -2.3909e-01\n",
      " -1.6375e-01  1.9977e-02 -1.0628e-01  5.5798e-02  1.4127e-01  4.6536e-01\n",
      " -3.3169e-01  1.8308e-01  2.9646e-01  2.3906e-02  3.2799e-01 -5.3632e-01\n",
      " -4.6895e-01 -1.7593e-02  4.6805e-03 -9.6152e-02 -1.2695e-01  6.4099e-02\n",
      " -2.9787e-01  3.7799e-01  4.4469e-01  1.2248e-01  7.6388e-02 -1.8102e-01\n",
      " -1.1795e-02  4.0090e-01 -3.6967e-01 -2.4106e-01 -4.2252e-01  2.1378e-01\n",
      "  4.0977e-01  1.3013e-01 -3.3478e-02  9.3179e-03  2.9553e-01  6.8702e-02\n",
      " -1.4949e-01  1.0473e-01  3.8860e-01 -3.7063e-01 -6.8934e-02  4.2111e-01\n",
      "  1.0861e-01  1.8585e-01 -1.1387e-01  2.0370e-01  9.4214e-02 -2.1426e-01\n",
      " -1.9376e-01  1.2261e-01  1.3971e-01 -5.4205e-01 -2.4502e-01  4.7454e-01\n",
      " -5.9380e-02 -1.2865e-01 -1.7345e-01  3.7465e-01 -7.2616e-02  3.1124e-01\n",
      " -3.3315e-04 -3.1445e-03  1.1435e-03 -8.0773e-02 -5.2824e-02  2.6108e-01\n",
      "  2.4586e-01 -5.8762e-02 -2.1999e-02  1.4060e-01  3.6004e-01 -8.1521e-02\n",
      " -2.8828e-02  2.3878e-01 -1.2243e-01  1.8758e-01  6.9072e-02 -6.8685e-02\n",
      "  1.2377e-01  3.1713e-02 -1.1530e-01  3.8205e-01  4.8575e-01 -1.6466e-01\n",
      "  1.1033e-01  1.3873e-01  3.2145e-01  2.8014e-02 -5.4760e-02 -1.9438e-01\n",
      " -1.5438e-02 -5.6435e-02  2.3037e-02  4.2915e-01  5.9080e-01  9.6298e-02\n",
      "  1.2788e-01  1.5916e-01 -7.2056e-02 -2.1036e-01  5.7346e-02  4.5768e-02\n",
      "  1.5730e-01  3.1019e-01 -4.2692e-02  1.7531e-02  2.9585e-02 -4.2749e-02\n",
      " -1.0006e-01 -3.1611e-01 -4.5054e-03 -1.3689e-01  4.0798e-01 -6.5866e-02\n",
      "  2.0162e-01 -7.9660e-02 -3.9495e-02  9.3723e-02  9.3557e-02 -9.7551e-02\n",
      "  3.0639e-01 -2.7325e-01 -3.3112e-01  3.4460e-02 -1.5027e-01  4.0673e-01]\n"
     ]
    }
   ],
   "source": [
    "a = []\n",
    "a = sentence_list[14]\n",
    "print(\"first from sentence 15:\", a.split()[0])\n",
    "a1=nlp(str(a.split()[0]))\n",
    "print(\"vector of first word:\",a1.vector)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
